# trawl.py
"""
Implementation of *trawl* processes â€“ a class of stationary stochastic
processes defined via a LÃ©vy basis ``L`` and a moving trawl set ``A_t``.
The key mathematical objects are:

* ``seed_cumulant`` â€“ a callable implementing the *log characteristic function*
  (i.e. the cumulant) of the LÃ©vy seed ``L'``.
* ``integrated_trawl_function`` â€“ the antiderivative of the trawl kernel
  ``g`` (denoted :math:`G(s) = âˆ«_s^{âˆž} g(Ï„) dÏ„`).

The concrete class :class:`GaussianTrawlProcess` (see ``gaussian_trawl.py``) is the
most common example.
"""

import torch
from torch import Tensor
from .base import StationaryStochasticProcess, StationaryProcessFDD
from typing import Callable, Optional
from abc import abstractmethod


class TrawlProcess(StationaryStochasticProcess):
    """
    Abstract base class for *trawl* processes.

    A trawl process is defined as

    .. math::
        X_t = L(A_t) = \\int_{A_t} \\! L'(\\mathrm{d}u),

    where ``L'`` is a LÃ©vy seed whose (log) characteristic function is
    supplied via ``seed_cumulant`` and ``A_t`` is a *trawl set* of the form
    .. math::
        A_t = \\{(x, t) |0 \\leq x \\leq g(-t)\\}
    governed by the *integrated trawl function* ``G`` such that ``G' = g``.

    Subâ€‘classes must implement a concrete probability density function (``pdf``)
    and a method to construct the finiteâ€‘dimensional distribution
    (:meth:`at_times`).

    Parameters
    ----------
    seed_cumulant
        Callable returning the *cumulant* of the LÃ©vy seed, i.e.

        ``seed_cumulant(u) = log E[exp(iÂ·uÂ·L')]``  (complexâ€‘valued).

        The function must accept a ``torch.Tensor`` of arbitrary shape and
        return a tensor of identical shape with ``dtype=torch.complex64`` or
        ``torch.complex128``.
    integrated_trawl_function
        Callable implementing the *integrated trawl function* :math:`G(s)`.
        It should accept a ``torch.Tensor`` (possibly negative) and return a
        realâ€‘valued ``torch.Tensor`` of the same shape.
    theta_batch_first, arg_check
        Forwarded to :class:`StationaryStochasticProcess`.
    """

    def __init__(
        self,
        seed_cumulant: Callable[[Tensor], Tensor],
        integrated_trawl_function: Callable[[Tensor], Tensor],
        *,
        theta_batch_first: bool = True,
        arg_check: bool = True,
    ) -> None:
        super().__init__(arg_check=arg_check, theta_batch_first=theta_batch_first)
        self.seed_cumulant = seed_cumulant  # â„“(u) = logâ€¯E[exp(iâ€¯uâ€¯Lâ€²)]
        self.it_func = integrated_trawl_function  # G(s) = âˆ«_s^âˆž g(Ï„) dÏ„

    # --------------------------------------------------------------------------
    # slice matrix (matrix of â€œlebesgue measure of slicesâ€™â€™ at fixed timestamps)
    # --------------------------------------------------------------------------
    def _compute_slice_partition(self, times: Tensor) -> Tensor:
        """
        Compute the *sliceâ€‘measure* matrix ``S`` for a given observation grid.

        The matrix ``S`` has shape ``(D, D)`` where ``D = len(times)``.
        Entry ``S[i, j]`` (with ``i â‰¤ j``) equals the Lebesgue measure of the
        intersection of the trawl sets ``A_{t_i}`` and ``A_{t_j}``.

        The implementation follows the vectorised â€œhackâ€™â€™.  It relies on the fact
        that the measure of the intersection can be expressed using the
        integrated trawl function evaluated at the four corner differences of the
        rectangle formed by the two times.

        Parameters
        ----------
        times
            1â€‘D tensor of increasing observation times.

        Returns
        -------
        Tensor
            Lowerâ€‘triangular sliceâ€‘measure matrix of shape ``(D, D)``.
        """
        D = times.shape[0]

        # Time differences required for the four corners of each rectangle.
        # The ``view``/``roll`` gymnastics give us a full pairwise matrix.
        t_diff_j_i = times.view(1, -1) - times.view(-1, 1)  # t_j - t_i
        t_diff_j_im1 = times.view(1, -1) - times.roll(1).view(-1, 1)  # t_j - t_{i-1}
        t_diff_jp1_i = times.roll(-1).view(1, -1) - times.view(-1, 1)  # t_{j+1} - t_i
        t_diff_jp1_im1 = times.roll(-1).view(1, -1) - times.roll(1).view(-1, 1)

        # The â€œfourâ€‘cornerâ€ formula (see the paper for a derivation).
        A = (
            self.it_func(t_diff_jp1_im1)
            - self.it_func(t_diff_jp1_i)
            + self.it_func(t_diff_j_i)
            - self.it_func(t_diff_j_im1)
        )
        # Edge corrections for the first row and last column.
        A[:, D - 1] = self.it_func(t_diff_j_i[:, D - 1]) - self.it_func(
            t_diff_j_im1[:, D - 1]
        )
        A[0, :] = self.it_func(t_diff_j_i[0, :]) - self.it_func(t_diff_jp1_i[0, :])
        A[0, D - 1] = self.it_func(times[D - 1] - times[0])

        # Transpose and retain the lowerâ€‘triangular part.
        return torch.tril(A.T)

    def _compute_cumulative_theta(self, theta: Tensor) -> Tensor:
        """
        Compute the *cumulative* version of ``theta`` required for the
        cumulant calculation of a trawl process.

        For a tensor ``theta`` of shape ``(B, D)`` the function returns a
        3â€‘D tensor ``C`` of shape ``(B, D, D)`` where

        .. math::
            C_{b,i,j} = \\sum_{k=0}^i ðŸ™_{k â‰¤ j}\\,\\theta_{b,k} .

        In other words, each slice ``C[b]`` contains lowerâ€‘triangular
        cumulative sums of the corresponding batch ``theta[b]``.

        Parameters
        ----------
        theta
            Tensor of shape ``(B, D)`` (batchâ€‘first layout).

        Returns
        -------
        Tensor
            Tensor of shape ``(B, D, D)`` containing the lowerâ€‘triangular
            cumulative sums.
        """
        B, D = theta.shape
        # unsqueeze: (B, D) â†’ (B, D, 1);
        # broadcast: (B, D) â†’ (B, D, D);
        # ``torch.tril(... )`` zeroes the upperâ€‘triangular part.
        return torch.tril(theta.unsqueeze(2).broadcast_to(B, D, D)).cumsum(1)

    def acf(self, lags: Tensor) -> Tensor:
        """
        Autocorrelation function for a trawl process.

        The (theoretical) ACF is proportional to the integrated trawl function:

        .. math::
            \\rho(â„“) = \\frac{G(â„“)}{G(0)}.

        Parameters
        ----------
        lags
            Nonâ€‘negative distance between two time points.

        Returns
        -------
        Tensor
            Autocorrelation values with the same shape as ``lags``.
        """
        return self.it_func(torch.abs(lags)) / self.it_func(torch.tensor(0.0))

    @abstractmethod
    def pdf(self, x: Tensor) -> Tensor:
        """Probability density function â€“ must be implemented by concrete subclasses."""
        raise NotImplementedError

    @abstractmethod
    def at_times(
        self, times: Tensor, rng: Optional[torch.Generator] = None
    ) -> "TrawlProcessFDD":
        """
        Create a :class:`TrawlProcessFDD` object for the supplied observation
        grid.

        Parameters
        ----------
        times
            Tensor of increasing observation times.
        rng
            Optional generator for reproducible sampling.

        Returns
        -------
        TrawlProcessFDD
            Finiteâ€‘dimensional distribution bound to ``self`` and ``times``.
        """
        raise NotImplementedError


class TrawlProcessFDD(StationaryProcessFDD):
    """
    Finiteâ€‘dimensional distribution (FDD) for a generic trawl process.

    It stores the observation times, a reference to the parent process and the
    *sliceâ€‘measure* matrix needed for cumulant evaluation.
    """

    def __init__(
        self,
        times: Tensor,
        process: TrawlProcess,
        rng: Optional[torch.Generator] = None,
    ) -> None:
        """
        Parameters
        ----------
        times
            Observation grid (must be strictly increasing).
        process
            Parent :class:`TrawlProcess` instance.
        rng
            Optional generator for deterministic sampling.
        """
        self.times = times
        self.process = process
        self.slices = self.process._compute_slice_partition(self.times)

        self.rng = rng

    # -----------------------------------------------------------------
    # internal helpers
    # -----------------------------------------------------------------
    @abstractmethod
    def _sample_slices(self, batch_size: int = 1) -> Tensor:
        """
        Sample the random *slice* variables that drive the trawl process.

        Concrete subclasses implement the sampling strategy appropriate for the
        underlying LÃ©vy seed (Gaussian, compound Poisson, etc.).

        Parameters
        ----------
        batch_size
            Number of independent copies to generate.

        Returns
        -------
        Tensor
            Tensor of shape ``(batch_size, D, D)`` where the entry
            ``[b, i, j]`` corresponds to the random increment associated
            with the intersection of the trawl sets ``A_{t_i}`` and ``A_{t_j}``.
        """
        raise NotImplementedError

    # -----------------------------------------------------------------
    # public API
    # -----------------------------------------------------------------
    def cumulant(
        self,
        theta: Tensor,
        theta_batch_first: Optional[bool] = None,
    ) -> Tensor:
        """
        Compute the joint cumulant ``Îº(Î¸)`` for the vector
        ``(X_{tâ‚}, â€¦, X_{t_D})`` associated with this FDD.

        The implementation follows the textbook formula

        .. math::
            Îº(Î¸) = \\sum_{i,j} \\ell\\bigl( C_{i,j}(Î¸) \\bigr) \\; S_{i,j}

        where ``â„“`` is the LÃ©vyâ€‘seed cumulant, ``C`` the cumulative theta
        matrix (computed by :meth:`TrawlProcess._compute_cumulative_theta`) and
        ``S`` the sliceâ€‘measure matrix.

        Parameters
        ----------
        theta
            Fourier arguments (shape ``(B, D)`` or ``(D, B)``).
        theta_batch_first
            Overrides the default orientation stored on the parent process.

        Returns
        -------
        Tensor
            Cumulant values of shape ``(B,)``.
        """
        # Normalise theta to ``(B, D)`` layout.
        theta_ = self.process._norm_theta(theta, theta_batch_first)

        # Compute the lowerâ€‘triangular cumulative theta matrix, evaluate the
        # LÃ©vyâ€‘seed cumulant on it and weight by the sliceâ€‘measure matrix.
        cumulants = (
            self.process.seed_cumulant(self.process._compute_cumulative_theta(theta_))
            * self.slices
        )
        # Sum over the two slice dimensions, leaving only the batch axis.
        return torch.sum(cumulants, dim=tuple(range(1, cumulants.ndim)))

    def charfunc(
        self,
        theta: Tensor,
        theta_batch_first: Optional[bool] = None,
    ) -> Tensor:
        """Characteristic function ``Ï†(Î¸) = exp(Îº(Î¸))``."""
        return torch.exp(self.cumulant(theta, theta_batch_first))

    def sample(
        self,
        batch_size: int = 1,
        batch_first: bool = True,
        unsqueeze_last: bool = True,
    ) -> Tensor:
        """
        Draw trajectories for the current observation grid.

        The construction proceeds by first sampling the *slice* variables
        (via :meth:`_sample_slices`) and then aggregating them in a cumulative
        fashion to obtain the process values at each observation time.

        Parameters
        ----------
        batch_size
            Number of independent copies to generate.
        batch_first
            If ``True`` (default) the output tensor has shape
            ``(batch_size, D, 1)``; otherwise the shape is ``(D, batch_size, 1)``.
        unsqueeze_last
            If ``True`` (default) a trailing singleton dimension is kept,
            matching the original library's output format.

        Returns
        -------
        Tensor
            Sampled trajectories of shape ``(batch_size, D, 1)`` (or transposed
            when ``batch_first=False``).
        """
        sec_length = self.times.shape[0]

        # Sample the ``(batch_size, D, D)`` slice matrix.
        slices_vals = self._sample_slices(batch_size=batch_size)

        # Initialise an empty tensor for the trajectories.
        traj = torch.zeros(
            (batch_size, sec_length), dtype=slices_vals.dtype, device=slices_vals.device
        )

        # For each observation time we accumulate the contributions of all slices
        # that intersect the corresponding trawl set.
        for i in range(sec_length):
            # ``slices_vals[:, i:, 0 : i + 1]`` extracts the appropriate lowerâ€‘triangular
            # block for time ``i``; summing twice yields the sum over both dimensions.
            traj[:, i] = (slices_vals[:, i:, 0 : i + 1]).sum(dim=1).sum(dim=1)

        if not batch_first:
            traj = traj.T
        if unsqueeze_last:
            traj = traj.unsqueeze(-1)

        return traj
